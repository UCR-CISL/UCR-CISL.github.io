@misc{bevcalib,
      title={BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations}, 
      author={Weiduo Yuan and Jerry Li and Justin Yue and Divyank Shah and Konstantinos Karydis and Hang Qiu},
      year={2025},
      eprint={2506.02587},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.02587}, 
      web={https://cisl.ucr.edu/BEVCalib/},
      img={bevcalib.png},
      code={https://github.com/UCR-CISL/BEVCalib},
}

@misc{talkingvehicles,
      title={Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play}, 
      author={Jiaxun Cui and Chen Tang and Jarrett Holtz and Janice Nguyen and Alessandro G. Allievi and Hang Qiu and Peter Stone},
      year={2025},
      eprint={2505.18334},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2505.18334},
      web={https://talking-vehicles.github.io/}, 
      img={talkingvehicles.png},
      code={https://github.com/cuijiaxun/talking-vehicles},
      demo={https://utexas.app.box.com/s/5pn2rfcprqv9p992m9lzyeaurq8irrhd},
}

@article{cats,
    title = {CATS: A Framework for Cooperative Autonomy Trust & Security},
    author = {Asavisanu, Namo and Khezresmaeilzadeh, Tina and Sequeira, Rohan and Qiu, Hang and Ahmad, Fawad and Psounis, Konstantinos and Govindan, Ramesh},
    journal = {IEEE Transactions on Vehicular Technology},
    publisher = {IEEE},
    series = {IEEE TVT},
    url = {https://arxiv.org/abs/2503.00659},
    img = {cats.png},
    year = {2025}
}

@article{cmp,
  author={Wang, Zehao and Wang, Yuping and Wu, Zhuoyuan and Ma, Hengbo and Li, Zhaowei and Qiu, Hang and Li, Jiachen},
  journal={IEEE Robotics and Automation Letters}, 
  title={CMP: Cooperative Motion Prediction with Multi-Agent Communication}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  keywords={Trajectory;Feature extraction;Three-dimensional displays;Robot sensing systems;Tracking;Predictive models;Laser radar;Transformers;Point cloud compression;Pipelines;Intelligent transportation systems;multi-robot systems;cooperating robots;cooperative prediction;connected and automated vehicles},
  doi={10.1109/LRA.2025.3546862},
  publisher={IEEE},
  img={cmp.png},
  web={https://cmp-cooperative-prediction.github.io/},
  url={https://arxiv.org/pdf/2403.17916},
  demo={https://www.youtube.com/watch?v=9y3ybSUqmO4},
  series = {IEEE RA-L},
}


@inproceedings{see_v2x,
    author = {Mo, Ruoshen and Wu, Bo and Tan, Zhaowei and Qiu, Hang},
    title = {SEE-V2X: C-V2X Direct Communication Dataset: An Application-Centric Approach},
    year = {2025},
    publisher = {Association for Computing Machinery},
    booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems},
    series = {ACM SenSys '25},
    img = {see-v2x.png},
    web = {https://cisl.ucr.edu/SEE-V2X/},
    code = {https://github.com/UCR-CISL/SEE-V2X/},
    data = {https://drive.google.com/drive/u/1/folders/1gAFsjYsjOiEH-U50wPoFzfS9-YYh_UwL}
}

@inproceedings{replayar,
    author = {Huang, Zijian and Shu, Cary and Qiu, Hang and Chen, Jiasi},
    title = {ReplayAR: A Tool for Visual Evaluation of Mixed Reality},
    year = {2024},
    isbn = {9798400704895},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3636534.3696213},
    doi = {10.1145/3636534.3696213},
    booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
    pages = {2106–2112},
    numpages = {7},
    keywords = {augemented reality, evaluation tools, visualization},
    location = {Washington D.C., DC, USA},
    series = {ACM MobiCom '24},
    note = {ImmerCom Workshop},
    img = {replayar.png},
    slides = {https://docs.google.com/presentation/d/1nlbcWakgK95wAiTLNMA8KBuDyriHCRjJeXleHAyhk9E/edit?usp=sharing},
    code = {https://github.com/mavens-lab/replayAR},
}

@inproceedings{harbor,
    author = {Zhu, Ruiyang and Zhu, Xiao and Zhang, Anlan and Zhang, Xumiao and Sun, Jiachen and Qian, Feng and Qiu, Hang and Mao, Z. Morley and Lee, Myungjin},
    title = {Boosting Collaborative Vehicular Perception on the Edge with Vehicle-to-Vehicle Communication},
    year = {2024},
    isbn = {9798400706974},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3666025.3699328},
    doi = {10.1145/3666025.3699328},
    abstract = {Collaborative Vehicular Perception (CVP) enables connected and autonomous vehicles (CAVs) to cooperatively extend their views through wirelessly sharing their sensor data. Existing CVP systems employ either a vehicle-to-vehicle (V2V) or vehicle-to-infrastructure (V2I) view exchange paradigm. In this paper, we advocate a hybrid CVP design: our developed system, Harbor, employs V2I as its fundamental underlying framework, and opportunistically employs V2V to boost the performance. In Harbor, vehicles (helpers) may serve as relays to assist other vehicles (helpees) in reaching an edge node, which performs sensor data merging to produce the extended view. We judiciously partition the workload between the edge and vehicles, develop a robust helper-helpee assignment model, and solve it efficiently at runtime. We conduct both real-world tests and large-scale emulation experiments using two prevailing CAV applications: drivable space detection and object detection. Our real-world evaluation conducted at one of the world's first purpose-built autonomous driving testbeds demonstrates that Harbor outperforms state-of-the-art V2V- or V2I-only CVP schemes by up to 36\% in detection accuracy, resulting in significantly fewer collisions under dangerous driving scenarios.},
    booktitle = {Proceedings of the 22nd ACM Conference on Embedded Networked Sensor Systems},
    pages = {141–154},
    numpages = {14},
    keywords = {cooperative vehicular sensing, vehicular networks, autonomous cars, LiDAR},
    location = {Hangzhou, China},
    series = {ACM SenSys '24},
    img = {harbor.png},
}

@inproceedings{elm,
    title={Embodied Understanding of Driving Scenarios},
    author={Zhou, Yunsong and Huang, Linyan and Bu, Qingwen and Zeng, Jia and Li, Tianyu and Qiu, Hang and Zhu, Hongzi and Guo, Minyi and Qiao, Yu and Li, Hongyang},
    booktitle = {Proceedings of The 18th European Conference on Computer Vision (ECCV)},
    series = {ECCV '24},
    year = {2024},
    code = {https://github.com/OpenDriveLab/ELM},
    data = {https://drive.google.com/drive/folders/1n4S0A4k8_9yDFIPIPWH_JLTUQ6yFc8ME?usp=sharing},
    url = {https://arxiv.org/abs/2403.04593},
    img = {elm.png},
}

@inproceedings{womdlidar,
    title={WOMD-LiDAR: Raw Sensor Dataset Benchmark for Motion Forecasting},
    author={Kan Chen and Runzhou Ge and Hang Qiu and Rami Ai-Rfou and Charles R. Qi and Xuanyu Zhou and Zoey Yang and Scott Ettinger and Pei Sun and Zhaoqi Leng and Mustafa Mustafa and Ivan Bogun and Weiyue Wang and Mingxing Tan and Dragomir Anguelov},
    year={2024},
    booktitle = {Proceedings of 2024 IEEE International Conference on Robotics and Automation},
    series = {ICRA '24},
    img = {womdlidar.png},
    url = {https://arxiv.org/abs/2304.03834},
    web = {https://waymo.com/research/womd-lidar/},
    data = {https://waymo.com/open/data/motion/},
    code = {https://github.com/waymo-research/waymo-open-dataset/blob/master/tutorial/tutorial_womd_lidar.ipynb},
    mediaa = {waymo-logo.svg},
    mediaaurl = {https://waymo.com/blog/2023/03/driving-research-forward-waymo-open.html},
}
@inproceedings{mcal,
    title={MCAL: Minimum Cost Human-Machine Active Labeling},
    author={Qiu, Hang and Chintalapudi, Krishna and Govindan, Ramesh},
    booktitle={Proceedings of the Eleventh International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=1FxRPKrH8bw},
    talk = {https://iclr.cc/virtual/2023/poster/12195},
    code = {https://github.com/hangqiu/MCAL},
    series = {ICLR '23},
    img = {mcal.jpg},
}

@inproceedings{qiu2021mlexray,
    title = {ML-EXray: Visibility into ML Deployment on the Edge},
    author = {Hang Qiu and Ioanna Vavelidou and Jian Li and Evgenya Pergament and Pete Warden and Sandeep Chinchali and Zain Asgar and Sachin Katti},
    year = {2022},
    url = {https://arxiv.org/abs/2111.04779},
    booktitle = {Proceedings of Machine Learning and Systems},
    series = {MLSys '22},
    img = {mlexray.png},
    code = {https://github.com/hangqiu/ML-EXray},
    award = {Outstanding Paper Award},
    mediaa = {embeddedcomputing.png},
    mediaaurl = {https://www.embeddedcomputing.com/technology/ai-machine-learning/ai-dev-tools-frameworks/ml-exray-a-cloud-to-edge-deployment-validation-framework},
}

@inproceedings{coopernaut,
    title = {Coopernaut: End-to-End Driving with Cooperative Perception for Networked Vehicles},
    author = {Hang Qiu* and Jiaxun Cui* and Dian Chen and Peter Stone and Yuke Zhu},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    series = {CVPR '22},
    month = {June},
    year = {2022},
    img = {coopernaut.png},
    web = {https://ut-austin-rpl.github.io/Coopernaut/},
    code = {https://github.com/UT-Austin-RPL/Coopernaut},
    demo = {https://youtu.be/rwR8Q5nuKkc},
    url = {https://arxiv.org/abs/2205.02222},
}


@inproceedings{autocast,
    title = {AutoCast: Scalable Infrastructure-less Cooperative Perception for Distributed Collaborative Driving},
    author = {Hang Qiu and Pohan Huang and Namo Asavisanu and Xiaochen Liu and Konstantinos Psounis and Ramesh Govindan},
    year = {2022},
    url = {https://arxiv.org/abs/2112.14947},
    web = {https://hangqiu.github.io/AutoCast/},
    code = {https://github.com/hangqiu/AutoCast},
    talk = {https://www.youtube.com/watch?v=EZ8t9eaddyM},
    img = {autocast.png},
    demo = {https://www.youtube.com/watch?v=uBmdCRmZNIo},
    booktitle = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
    series = {ACM MobiSys '22},
}

@software{mlexray_pip,
    title = {ML-EXray Python package},
    url = {https://test.pypi.org/project/MLEXray/},
    year = {2021}
}

@article{CamLoc,
author = {Ghosh, Pradipta and Liu, Xiaochen and Qiu, Hang and Vieira, Marcos A. M. and Sukhatme, Gaurav S. and Govindan, Ramesh},
title = {Sensing the Sensor: Estimating Camera Properties with Minimal Information},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3508393},
doi = {10.1145/3508393},
journal = {ACM Transactions on Sensor Networks},
month = {feb},
articleno = {28},
numpages = {26},
keywords = {neural network, Localization, camera, mechanical turk, vanishing point},
series = {TOSN '22},
img = {CamLoc.png},
}

@techreport{ghosh2020localizing,
    title = {On Localizing a Camera from a Single Image},
    author = {Ghosh, Pradipta and Liu, Xiaochen and Qiu, Hang and Vieira, Marcos AM and Sukhatme, Gaurav S and Govindan, Ramesh},
    url = {https://arxiv.org/abs/2003.10664},
    year = {2020},
    institution = {ArXiv}
}

@techreport{jiang2013flexible,
    title = {Flexible and Efficient Sensor Fusion for Automotive Apps},
    author = {Jiang, Yurong and Qiu, Hang and McCartney, Matthew and Halfond, William GJ and Bai, Fan and Grimm, Donald and Govindan, Ramesh},
    year = {2013},
    institution = {Citeseer},
    url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.723.5267&rep=rep1&type=pdf}
}

@inbook{avr_getmobile,
    author = {Qiu, Hang and Ahmad, Fawad and Bai, Fan and Gruteser, Marco and Govindan, Ramesh},
    title = {Augmented Vehicular Reality: Enabling Extended Vision for Future Automobiles},
    year = {2019},
    issue_date = {December 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {22},
    number = {4},
    issn = {2375-0529},
    url = {https://doi.org/10.1145/3325867.3325880},
    doi = {10.1145/3325867.3325880},
    booktitle = {GetMobile: Mobile Comp. and Comm.},
    month = {may},
    pages = {30–34},
    numpages = {5},
    award = {Invited article (Best Paper runner-up of Mobisys’18)},
}

@inbook{ahmad2018quicksketch,
    title = {QuickSketch: Building 3D Representations in Unknown Environments using Crowdsourcing},
    author = {Ahmad, Fawad and Qiu, Hang and Liu, Xiaochen and Bai, Fan and Govindan, Ramesh},
    booktitle = {21st International Conference on Information Fusion (Fusion '18)},
    pages = {2314--2321},
    year = {2018},
    organization = {IEEE},
    url = {https://ieeexplore.ieee.org/abstract/document/8455437/},
}

@inbook{icm,
    title = {My Story with ICM},
    author = {	Chen,Xilun and Qiu,Hang and Yang, Chunzhi },
    booktitle = {UMAP Journal 34.2 \& 34.3 Summer \/ Fall 2013 Edition - The 2013 MCM \/ ICM Contest Edition},
    year = {2013},
    url = {https://www.comap.com/undergraduate/contests/stories/My_Story_with_ICM.pdf},
    award = {Invited article (Outstanding Winner of ICM’12)}
}

@inproceedings{avr,
    author = {Qiu, Hang and Ahmad, Fawad and Bai, Fan and Gruteser, Marco and Govindan, Ramesh},
    title = {AVR: Augmented Vehicular Reality},
    year = {2018},
    isbn = {9781450357203},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3210240.3210319},
    talk = {https://youtu.be/QWrEl1-m0ho},
    demo = {https://www.youtube.com/watch?v=9rOtH3hDcw8&t},
    code = {https://github.com/hangqiu/AVR16},
    doi = {10.1145/3210240.3210319},
    booktitle = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
    pages = {81–95},
    numpages = {15},
    keywords = {Collaborative Sensing, Autonomous Cars, Extended Vision},
    location = {Munich, Germany},
    series = {ACM MobiSys '18},
    award = {Best Paper Runner-up Award},
    img = {avr.png}
}

@inproceedings{avr_hotmobile,
    author = {Qiu, Hang and Ahmad, Fawad and Govindan, Ramesh and Gruteser, Marco and Bai, Fan and Kar, Gorkem},
    title = {Augmented Vehicular Reality: Enabling Extended Vision for Future Vehicles},
    year = {2017},
    isbn = {9781450349079},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3032970.3032976},
    doi = {10.1145/3032970.3032976},
    abstract = {Like today's autonomous vehicle prototypes, vehicles in the future will have rich sensors to map and identify objects in the environment. For example, many autonomous vehicle prototypes today come with line-of-sight depth perception sensors like 3D cameras. These cameras are used for improving vehicular safety in autonomous driving, but have fundamentally limited visibility due to occlusions, sensing range, and extreme weather and lighting conditions. To improve visibility and performance, not just for autonomous vehicles but for other Advanced Driving Assistance Systems (ADAS), we explore a capability called Augmented Vehicular Reality (AVR). AVR broadens the vehicle's visual horizon by enabling it to share visual information with other nearby vehicles, but requires careful techniques to align coordinate frames of reference, and to detect dynamic objects. Preliminary evaluations hint at the feasibility of AVR and also highlight research challenges in achieving AVR's potential to improve autonomous vehicles and ADAS.},
    booktitle = {Proceedings of the 18th International Workshop on Mobile Computing Systems and Applications},
    pages = {67–72},
    numpages = {6},
    keywords = {Extended Vision, Collaborative Sensing, Autonomous Cars, ADAS},
    location = {Sonoma, CA, USA},
    series = {ACM HotMobile '17},
    img = {avr_hotmobile.png}
}

@inproceedings{10.1145/2668332.2668350,
    author = {Jiang, Yurong and Qiu, Hang and McCartney, Matthew and Halfond, William G. J. and Bai, Fan and Grimm, Donald and Govindan, Ramesh},
    title = {CARLOG: A Platform for Flexible and Efficient Automotive Sensing},
    year = {2014},
    isbn = {9781450331432},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2668332.2668350},
    doi = {10.1145/2668332.2668350},
    booktitle = {Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems},
    pages = {221–235},
    numpages = {15},
    keywords = {datalog, automotive, predicate acquisition, latency},
    location = {Memphis, Tennessee},
    series = {ACM SenSys '14},
    img = {carlog.png}
}

@inproceedings{10.1145/2809695.2809725,
    author = {Jiang, Yurong and Qiu, Hang and McCartney, Matthew and Sukhatme, Gaurav and Gruteser, Marco and Bai, Fan and Grimm, Donald and Govindan, Ramesh},
    title = {CARLOC: Precise Positioning of Automobiles},
    year = {2015},
    isbn = {9781450336314},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2809695.2809725},
    doi = {10.1145/2809695.2809725},
    abstract = {Precise positioning of an automobile to within lane-level precision can enable better navigation and context-awareness. However, GPS by itself cannot provide such precision in obstructed urban environments. In this paper, we present a system called CARLOC for lane-level positioning of automobiles. CARLOC uses three key ideas in concert to improve positioning accuracy: it uses digital maps to match the vehicle to known road segments; it uses vehicular sensors to obtain odometry and bearing information; and it uses crowd-sourced location of estimates of roadway landmarks that can be detected by sensors available in modern vehicles. CARLOC unifies these ideas in a probabilistic position estimation framework, widely used in robotics, called the sequential Monte Carlo method. Through extensive experiments on a real vehicle, we show that CARLOC achieves sub-meter positioning accuracy in an obstructed urban setting, an order-of-magnitude improvement over a high-end GPS device.},
    booktitle = {Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems},
    pages = {253–265},
    numpages = {13},
    keywords = {map, GPS, accuracy},
    location = {Seoul, South Korea},
    series = {ACM SenSys '15},
    img = {carloc.png}
}

@inproceedings{qiu2018kestrel,
    title = {Kestrel: Video analytics for augmented multi-camera vehicle tracking},
    author = {Qiu, Hang and Liu, Xiaochen and Rallapalli, Swati and Bency, Archith J and Chan, Kevin and Urgaonkar, Rahul and Manjunath, BS and Govindan, Ramesh},
    booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Internet-of-Things Design and Implementation},
    series = {IEEE/ACM IoTDI '18},
    pages = {48--59},
    year = {2018},
    organization = {IEEE},
    url = {https://ieeexplore.ieee.org/abstract/document/8366976},
    demo = {https://youtu.be/vSO7mYUpEhs},
    data = {https://drive.google.com/file/d/0Bz3GHg4Beq4Xd1ZjUzk2dG1ycEMyZ3FGaXNLV2FLN0RPeGNR/view?usp=sharing},
    img = {kestrel.png},
}

@article{qiu2017towards,
    title = {Towards Robust Vehicular Context Sensing},
    author = {Qiu, Hang and Chen, Jinzhu and Jain, Shubham and Jiang, Yurong and McCartney, Matt and Kar, Gorkem and Bai, Fan and Grimm, Donald K and Gruteser, Marco and Govindan, Ramesh},
    journal = {IEEE Transactions on Vehicular Technology},
    volume = {67},
    number = {3},
    pages = {1909--1922},
    year = {2017},
    publisher = {IEEE},
    series = {IEEE TVT},
    url = {https://ieeexplore.ieee.org/abstract/document/8100990},
    data = {https://drive.google.com/file/d/16V731K8WYizVcZPywPjKLlGPhsJkW-l4/view?usp=sharing},
    img = {towards_tvt.png},
}

@inproceedings{10.1145/2942358.2942372,
    author = {Qiu, Hang and Psounis, Konstantinos and Caire, Giuseppe and Chugg, Keith M. and Wang, Kaidong},
    title = {High-Rate WiFi Broadcasting in Crowded Scenarios via Lightweight Coordination of Multiple Access Points},
    year = {2016},
    isbn = {9781450341844},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2942358.2942372},
    doi = {10.1145/2942358.2942372},
    booktitle = {Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
    pages = {301–310},
    numpages = {10},
    keywords = {802.11, cooperative transmission, access point coordination, WiFi access in crowded scenarios},
    location = {Paderborn, Germany},
    series = {ACM MobiHoc '16},
    talk = {https://youtu.be/G_F1DkKam3Q},
    slides = {https://nsl.usc.edu/wp-content/uploads/2017/09/co-bcast_mobihoc16.pptx},
    img = {cobcast.png},
}

@inproceedings{carmap,
    author = {Fawad Ahmad and Hang Qiu and Ray Eells and Fan Bai and Ramesh Govindan},
    title = {CarMap: Fast 3D Feature Map Updates for Automobiles },
    booktitle = {Proceedings of the 17th Symposium on Networked Systems Design and Implementation},
    year = {2020},
    series = {USENIX NSDI '20},
    isbn = {978-1-939133-13-7},
    address = {Santa Clara, CA},
    pages = {1063--1081},
    url = {https://www.usenix.org/system/files/nsdi20-paper-ahmad.pdf},
    demo = {https://youtu.be/SlG4QGq5ypk},
    code = {https://github.com/USC-NSL/CarMap},
    publisher = {{USENIX} Association},
    month = {Feb},
    img = {carmap.png}
}

%@inproceedings{lee2018tracking,
%  title={On Tracking Realistic Targets in a Megacity with Contested Air and Spectrum Access},
%  author={Lee, Jongdeog and Abdelzaher, Tarek and Qiu, Hang and Govindan, Ramesh and Marcus, Kelvin and Hobbs, Reginald and Suri, Niranjan and Dron, Will},
%  year={2018},
%  booktitle={Proceedings of the 37th Military Communications Conference},
%  series={MILCOM '18},
%  url={https://ieeexplore.ieee.org/abstract/document/8599773}
%}

@inproceedings{he2020fedml,
    title = {FedML: A Research Library and Benchmark for Federated Machine Learning},
    author = {He, Chaoyang and Li, Songze and So, Jinhyun and Zhang, Mi and Wang, Hongyi and Wang, Xiaoyang and Vepakomma, Praneeth and Singh, Abhishek and Qiu, Hang and Shen, Li and others},
    booktitle = {Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS '20), Workshop on Scalability, Privacy, and Security in Federated Learning},
    series = {NeurIPS, SpicyFL '20},
    year = {2020},
    award = {Best Paper Award},
    url = {https://arxiv.org/abs/2007.13518},
    web = {https://fedml.ai/},
    img = {fedml.png},
    code = {https://github.com/FedML-AI},
}

@patent{hdmap19,
    title = {Crowd-sensed Point Cloud Map},
    author = {Ahmad, Fawad and Qiu, Hang and Bai, Fan and Govindan, Ramesh},
    booktitle = {US20190266748 / CN110186467 / DE102019104482},
    award = {Worldwide Patent:},
    url = {https://patents.google.com/patent/US10529089B2/en},
    year = {2019}
}

@patent{avr18,
    title = {Method and Apparatus of Networked Scene Rendering and Augmentation in Vehicular Environments in Autonomous Driving Systems},
    author = {Qiu, Hang and Govindan, Ramesh and Gruteser, Marco and Bai, Fan},
    booktitle = {US20180261095 / CN108574929 / DE102018105293},
    award = {Worldwide Patent:},
    url = {https://patents.google.com/patent/US10109198B2/en},
    year = {2018}
}

@inproceedings{wheatman2020optimal,
    title = {Optimal Resource Allocation for Crowdsourced Image Processing},
    author = {Wheatman, Kristina Sorensen and Mehmeti, Fidan and Mahon, Mark and Qiu, Hang and Chan, Kevin and La Porta, Thomas},
    booktitle = {Proceedings of the 17th Annual IEEE International Conference on Sensing, Communication, and Networking},
    pages = {1--9},
    year = {2020},
    organization = {IEEE},
    series = {IEEE SECON '20},
    url = {https://ieeexplore.ieee.org/abstract/document/9158417},
    img = {secon20.png}
}

@patent{carmap20,
    title = {Method and Apparatus for a Context-aware Crowd-sourced Sparse High Definition Map},
    author = {Ahmad, Fawad and Qiu, Hang and Govindan, Ramesh and Grimm, Donald K and Bai, Fan},
    booktitle = {US20200278217 / CN111638536 / DE102020102725},
    award = {Worldwide Patent:},
    url = {https://patents.google.com/patent/US20200278217A1/en},
    year = {2020}
}

@patent{mrmc,
    title = {Greedy Channel-allocation in Multi-radio Multi-channel Multi-hop Wireless Network},
    author = {Qiu, Hang and Huang, Xin and Shi, Qi and Wang, Xinbing and Tian, Jun },
    booktitle = {Patent: CN103634846},
    url = {https://patents.google.com/patent/CN103634846B/ko},
    year = {2012},
}


@patent{xin12,
    title = {Energy-efficient Cooperative Sensing Schedule for Heterogeneous Users in Cognitive Radio Network},
    author = {Huang, Xin and Feng,Xinxin and Qiu, Hang and Sun,Gaofei and Tian,Xiaohua and Yang,Feng and Wang, Xinbing},
    booktitle = {Patent: CN102905381},
    url = {https://patents.google.com/patent/CN102905381B/tr},
    year = {2012}
}

@patent{wax_12,
    title = {Automatic Line-tracking Floor Waxing Machine},
    author = {Qiu, Hang and Huang, Xin},
    booktitle = {Patent: CN202458213},
    url = {http://cisip.org.cn/portal/patentDetail/CN201220102332.7.html},
    year = {2011}
}

%@misc{qiu2020minimum,
%  title={Minimum Cost Active Labeling},
%  author={Qiu, Hang and Chintalapudi, Krishna and Govindan, Ramesh},
%  journal={Under submission},
%  year={2022},
%  url={https://arxiv.org/abs/2006.13999}
%}

%@misc{qiu2018satyam,
%  title={Satyam: Democratizing Groundtruth for Machine Vision},
%  author={Qiu, Hang and Chintalapudi, Krishna and Govindan, Ramesh},
%  journal={Integrated into Microsoft Azure ML. Used by UCSB, USC, UIUC, ARL.},
%  year={2018},
%  award={Featured in Microsoft Ignite 2019},
%  url={https://arxiv.org/abs/1811.03621}
%}

@software{fourseasons_code,
    title = {FourSeasons Dataset and Benchmark},
    url = {https://trafficcamdataset.wordpress.com/},
    year = {2020}
}

@software{satyam_code,
    title = {Satyam Github},
    url = {https://github.com/satyamresearch/satyam},
    year = {2020}
}

@software{AVR_code,
    title = {AVR Github},
    url = {https://github.com/hangqiu/AVR16},
    year = {2018}
}

@software{AutoCast_code,
    title = {Scalable Cooperative Perception Github},
    url = {https://github.com/hangqiu/AutoCast},
    year = {2020}
}

@software{CarMap_code,
    title = {CarMap Github},
    url = {https://github.com/USC-NSL/CarMap},
    year = {2020}
}


